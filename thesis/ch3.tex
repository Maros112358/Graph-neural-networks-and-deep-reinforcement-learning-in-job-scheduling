\chapter{Published models}

This chapter will present five publicly available job scheduling models based on deep reinforcement learning on graph neural networks. Three models are designed for JSSP, and two are for FJSP. For models designed for JSSP, our extension to JSSP is also presented. 

\section{JSSP Models}

\subsection{L2D}
\textbf{L2D} is a model published in \cite{zhang2020learning}. To represent the JSSP, it employs a modified disjunctive graph described in \ref{JSSP as a disjunctive graph}, where the authors start with the original disjunctive graph containing only conjunctive arcs, and for each disjunctive edge, they add a conjunctive arc representing new precedence constraint. This design is because replacing each disjunctive edge with a pair of opposite conjunctive arcs results in a too-dense, fully oriented graph \cite{zhang2020learning}.
\par
To parametrize the policy $\pi_\theta(a_t|s_t)$, GIN for oriented graphs described in \ref{graph Isomorphism network} is used to obtain graph node embeddings $h_o^{(T)}$ in the message passing phase. Initial node features were a 2-elementer vector $h_o^{(0)} = (I(o), C_{LB}(o))$, where $I(o)$ is equal to 1 only if $o \in O$ is scheduled, otherwise 0, and $C_{LB}(o)$ is the lower bound of the estimated time of completion. 
\par
To select the action $a_t$ at $s_t$ in the readout phase, obtained graph embeddings are pooled using average pooling, i.e., $h_G = \frac{1}{|O|} \sum_{o \in O} h_o^{(T)}$, concatenated with the embedding of the eligible operation $o$ corresponding to action $a_t$, and passed through a multi-layer perceptron to obtain a score for the corresponding action $scr(a_t) = \text{MLP}_{\pi_\theta}\left ( \left [h_o^{(T)} || h_G \right ] \right )$. The softmax function is then applied to obtain a distribution over available actions $P(a_t)$ from the scores \cite{zhang2020learning}.\\
\\
To train the policy, authors used Proximal Policy Optimization (PPO) \cite{DBLP:journals/corr/SchulmanWDRK17}. 

\subsubsection{Extending L2D to DJSP}

\xxx{here I will add the extension of l2d to the dynamic version of JSSP}

\subsection{IEEE-ICCE-RL-JSP}

\textbf{IEEE-ICCE-RL-JSP} was published in \cite{10226873}. Authors represent JSSP as a heterogeneous graph discussed in \ref{FJSP as a heterogenous graph}. In each step, this model chooses one of the traditional PDRs to determine the operation to dispatch.
\par
To parametrize the policy, authors used GIN on heterogeneous graphs discussed in \ref{graph Isomorphism network} in the message passing phase. Features of operation nodes include the status of the operation, the processing time, the remaining time of the operation, and the number of remaining operations in the current job. Features of machine nodes include the status of the corresponding machine and the remaining time of the operation being processed \cite{10226873}.
\par
In the readout phase, obtained embeddings are pooled using sum pooling and fed into a value network based on DDQN, which outputs a score value for each PDR. The agent then selects a PDR with a maximum score \cite{10226873}.

\subsubsection{Extending IEEE-ICCE-RL-JSP to DJSP}

\xxx{here I will add the extension of IEEE-ICCE-RL-JSP to the dynamic version of JSSP}