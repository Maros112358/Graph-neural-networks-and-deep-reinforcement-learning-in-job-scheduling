\chapter{Experimental comparison}

In this chapter, we will experimentally compare the models presented in the previous chapter. We will describe the experimental setup for each job scheduling variant (JSSP, FJSP, DJSP) and interpret the results. 

\section{Experimental setup}

\subsection{Instances}

\subsubsection*{JSSP}

To compare models capable of solving JSSP, we obtained benchmark JSSP instances from \cite{jssp_benchmarks}. Each instance is a text file describing a JSSP instance using Taillard Specification \cite{taillard_specification}, where there are two numbers on the first line: the number of jobs $|\mathcal{J}|$ and the number of machines $|\mathcal{M}|$. Then, on the next $|\mathcal{J}|$ lines are operation processing times $p_{ij}$ with one line corresponding to one job. The next $|\mathcal{J}|$ lines describe on which machines should operations be processed \cite{jssp_benchmarks}. This text file is fed to each model, and the resulting schedule and makespan are obtained.  An example of Taillard's Specification is shown below \cite{jssp_benchmarks}.
\begin{verbatim}
    1   3
    6   7  5
    2   3  1    
\end{verbatim}
In this example, there is one job and three machines. The first operation has a processing time of 6 and is processed on the second machine. The second operation is processed on the third machine with a processing time of 7, and the last operation is processed on the first machine with a processing time of 5.
\par
Since FJSP models take only FJSP instances as an input, we reformulate each JSSP instance as the FJSP instance with $|\mathcal{M}_{ij}| = 1$, whose text representation will be described in the following subsection. 
\par
To test the sensitivity of each model, we shuffle the order of jobs in each text file with 10 different seeds before feeding it to the model. Changing the order of jobs in the text file does not change the underlying instance itself, only its text representation; therefore, shuffling should not affect the model. However, during the experiment, we observed that some models give different results when we shuffle the text file.

\subsubsection*{FJSP}

To compare two models capable of solving FJSP presented in the previous chapter, we obtained 402 benchmark FJSP instances from \cite{fjsp_benchmarks}. Each instance is again a text file, where on the first line are at least two numbers: the number of jobs, the number of machines, and the last number corresponding to the average number of machines per operation is optional. Then, the next $|\mathcal{J}|$ lines represent one job. The first number is the number of operations in that job; the second number is the number of machines $k$ that can process the first operation, and then there are $k$ pairs of numbers corresponding to the eligible machine and its processing time of the given operation. This is followed by the data for the second operation, and so on. An example of an FJSP instance is shown below.
\begin{verbatim}
    1   3
    3   1   2   6   1   3   7   1   1   5   
\end{verbatim}
This FJSP instance example has one job with three operations. The first operation can be processed only on the second machine, the second operation can only be processed on the third machine, and the last operation can only be processed on the first machine. This FJSP instance is the JSSP instance example described above reformulated as an FJSP instance.
\par 
This text file is then fed to the model, and we obtain the schedule and the makespan. Again, to test the model's sensitivity, we shuffled the order of jobs with 10 different seeds.

\subsubsection*{DJSP}
To compare our extensions of JSSP models to DJSP presented in the previous chapter, we designed an experiment inspired by \cite{djsp_experiment_design}. We assume that there is a set of jobs known at the beginning. We then model the arrival of new jobs as a Poisson process, i.e., the arrival of two consecutive jobs follows an exponential distribution \cite{djsp_experiment_design}. We model the average arrival time as 
\begin{equation}
    \Delta t_\text{avg} = \frac{\mu_a}{U} \, ,
\end{equation}
where $\mu_a$ is the average processing time of all operations, and $U$ is a load factor of the dynamic job shop. We use $U = \{1,2,4 \}$ in the experiment. 
\par
To generate the set of known and arriving jobs, we take the JSSP benchmark instance, shuffle the order of jobs in the text file, and use the first half of the shuffled instance as known jobs and the rest as arriving jobs. Again, we use 10 different seeds to test the model's sensitivity.


\subsection{Models}
\subsubsection{L2D}
Authors of \textbf{L2D} trained \textbf{L2D} on randomly generated JSSP instances \cite{zhang2020learning}. They trained multiple parameter sets to examine the ability of the \text{L2D} to generalize to bigger JSSP instances. Each parameter set was trained on random instances of constant size. From their GitHub repository \cite{github_l2d}, we obtained parameter sets trained on instances with size 6x6, 10x10, 15x15, 20x15, 20x20, 30x15, 30x20. Number of trainable parameters in each parameter set is 19458. In our experiments, we use each obtained parameter set. 
\par
In each parameter set, each $\text{MLP}^{(k)}$ has two hidden layers, each with 64 neurons. The action selection and the value prediction networks have two hidden layers, each with 32 neurons. In PPO, the clipping parameter was set to $\epsilon = 0.2$. Number of GIN layers was set to $K = 2$. Parameters update was done using Adam optimizer with learning rate $2\times10^{-5}$. For specific details of the training process, we refer the reader to the supplementary material of \cite{zhang2020learning}.

\subsubsection{Wheatley}

\subsubsection{IEEE-ICCE-RL-JSP}
Authors of \text{IEEE-ICCE-RL-JSP} trained the model on randomly generated 3x3 and 10x10 JSSP instances. Parameter updates are done using Adam optimizer with a $10^{-5}$ learning rate. As PDRs for agents to select in each step, authors used FIFO, MOR, SPT, and MWKR discussed in \ref{priority dispatching rules}. The number of layers in HGIN was set to $K=3$. The value network has three dense layers. All hidden layers in both networks have dimension 128.
\par
No parameter sets were found in the GitHub repository\cite{github_ieee_icce_rl_jsp}, so we had to train our own parameter sets using a script found in the repository. The script trains the model until the \textit{gap} between the best-known solution and the solution given by the model is not smaller than 20$\%$. We trained five parameter sets.
