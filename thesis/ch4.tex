\chapter{Experimental comparison}

In this chapter, we will experimentally compare the models presented in the previous chapter. We will describe the experimental setup for each job scheduling variant (JSSP, FJSP, DJSP) and interpret the results. 

\section{Experimental setup}

\subsection{Instances}

\subsubsection*{JSSP}

To compare models capable of solving JSSP, we obtained 242 benchmark JSSP instances and their best known solutions from \cite{jssp_benchmarks}. Each instance is a text file describing a JSSP instance using Taillard Specification \cite{taillard_specification}, where there are two numbers on the first line: the number of jobs $|\mathcal{J}|$ and the number of machines $|\mathcal{M}|$. Then, on the next $|\mathcal{J}|$ lines are operation processing times $p_{ij}$ with one line corresponding to one job. The next $|\mathcal{J}|$ lines describe on which machines should operations be processed \cite{jssp_benchmarks}. This text file is fed to each model, and the resulting schedule and makespan are obtained.  An example of Taillard's Specification is shown below \cite{jssp_benchmarks}.
\begin{verbatim}
    1   3
    6   7  5
    2   3  1    
\end{verbatim}
In this example, there is one job and three machines. The first operation has a processing time of 6 and is processed on the second machine. The second operation is processed on the third machine with a processing time of 7, and the last operation is processed on the first machine with a processing time of 5.
\par
Since FJSP models take only FJSP instances as an input, we reformulate each JSSP instance as the FJSP instance with $|\mathcal{M}_{ij}| = 1$, whose text representation will be described in the following subsection. 
\par
For each model and each JSSP instance, we run the experiment with 10 different seeds. Besides makespan, we are also interested in \textit{gap}, which is given by the following equation
\begin{equation}
    gap  = \frac{C - C_\text{best}}{C_\text{best}} \, ,
\end{equation}
where $C$ is the makespan produced by the model or PDR, and $C_\text{best}$ is the best-known makespan from the literature \cite{jssp_benchmarks}.
\par
We also separated JSSP instances into three categories based on the number of jobs. Instances with less than 20 jobs are \textit{"small"}, instances with at least 50 jobs are \textit{"large"}, and \textit{"medium"} otherwise.

\subsubsection*{FJSP}

To compare two models capable of solving FJSP presented in the previous chapter, we obtained 402 benchmark FJSP instances and their best known solutions from \cite{fjsp_benchmarks}. Each instance is again a text file, where on the first line are at least two numbers: the number of jobs, the number of machines, and the last number corresponding to the average number of machines per operation is optional. Then, the next $|\mathcal{J}|$ lines represent one job. The first number is the number of operations in that job; the second number is the number of machines $k$ that can process the first operation, and then there are $k$ pairs of numbers corresponding to the eligible machine and its processing time of the given operation. This is followed by the data for the second operation, and so on. An example of an FJSP instance is shown below.
\begin{verbatim}
    1   3
    3   1   2   6   1   3   7   1   1   5   
\end{verbatim}
This FJSP instance example has one job with three operations. The first operation can be processed only on the second machine, the second operation can only be processed on the third machine, and the last operation can only be processed on the first machine. This FJSP instance is the JSSP instance example described above reformulated as an FJSP instance.
\par 
This text file is then fed to the model, and we obtain the schedule and the makespan. Again, we repeat the experiment for each model and each FJSP instance with 10 different seeds.

\subsubsection*{DJSP}
To compare our extensions of JSSP models to DJSP presented in the previous chapter, we designed an experiment inspired by \cite{djsp_experiment_design}. We assume that there is a set of jobs known at the beginning. We then model the arrival of new jobs as a Poisson process, i.e., the arrival of two consecutive jobs follows an exponential distribution \cite{djsp_experiment_design}. We model the average arrival time as 
\begin{equation}
    \Delta t_\text{avg} = \frac{\mu_a}{U} \, ,
\end{equation}
where $\mu_a$ is the average processing time of all operations, and $U$ is a load factor of the dynamic job shop. We use $U = \{1,2,4 \}$ in the experiment. 
\par
To generate the set of known and arriving jobs, we take the JSSP benchmark instance, shuffle the order of jobs in the text file, and use the first half of the shuffled instance as known jobs and the rest as arriving jobs. Again, we repeat each experiment with 10 different seeds.

\subsection{Models}
\subsubsection{L2D}
Authors of \textbf{L2D} trained \textbf{L2D} on randomly generated JSSP instances \cite{zhang2020learning}. They trained multiple parameter sets to examine the ability of the \text{L2D} to generalize to bigger JSSP instances. Each parameter set was trained on random instances of constant size. From their GitHub repository \cite{github_l2d}, we obtained parameter sets trained on instances with size 6x6, 10x10, 15x15, 20x15, 20x20, 30x15, 30x20. Number of trainable parameters in each parameter set is 19458. In our experiments, we use each obtained parameter set. 
\par
In each parameter set, each $\text{MLP}^{(k)}$ has two hidden layers, each with 64 neurons. The action selection and the value prediction networks have two hidden layers, each with 32 neurons. In PPO, the clipping parameter was set to $\epsilon = 0.2$. Number of GIN layers was set to $K = 2$. Parameters update was done using Adam optimizer with learning rate $2\times10^{-5}$. For specific details of the training process, we refer the reader to the supplementary material of \cite{zhang2020learning}.

\subsubsection{Wheatley}
For the experiment, we had to train \textbf{Wheatley} ourselves. To make \textbf{Wheatley} as similar to \textbf{L2D} as possible, we chose the same training hyperparameters as \text{L2D}. \textbf{Wheatley} has one extra training Hyperparameter specifying the maximum number of jobs and machines that the model will be able to process. 
\par
We trained three parameter sets capable of solving JSSP and one capable of solving DJSP. The first parameter set was trained on randomly generated 30x20 instances, capable of solving instances of size up to 100x20. The second parameter set was trained on randomly generated 20x20 instances, capable of solving instances of size up to 100x20. The last third parameter set was trained on randomly generated 20x20 instances and is able to solve instances of size up to \textbf{100x70} because, as discussed in \ref{dynamicwheatley}, for each arriving job, we add a new machine. And since the maximum number of jobs is 100, and half of them will be known at the start, we will have at most 70 machines. As a result, we used only the last parameter set for DJSP.
\par
The listed parameter sets were trained using a CPU on an Arm-based Ampere A1 virtual machine with 4 CPUs and 24 GB of RAM in Oracle Cloud Infrastructure with Ubuntu 20.04 64-bit operating system. We stopped the training when the model had not improved its objective for at least two days. Each model was trained for at least 3 weeks. 

\subsubsection{IEEE-ICCE-RL-JSP}
Authors of \textbf{IEEE-ICCE-RL-JSP} trained the model on randomly generated 3x3 and 10x10 JSSP instances. Parameter updates are done using Adam optimizer with a $10^{-5}$ learning rate. As PDRs for agents to select in each step, authors used FIFO, MOR, SPT, and MWKR discussed in \ref{priority dispatching rules}. The number of layers in HGIN was set to $K=3$. The value network has three dense layers. All hidden layers in both networks have dimension 128.
\par
No parameter sets were found in the GitHub repository\cite{github_ieee_icce_rl_jsp}, so we had to train our own parameter sets using a script found in the repository. The script trains the model until the \textit{gap} between the best-known solution and the solution given by the model is not smaller than 20$\%$. We trained five parameter sets. We trained the model using CPU on an Arm-based Ampere A1 virtual machine with 4 CPUs and 24 GB of RAM in Oracle Cloud Infrastructure with Ubuntu 20.04 64-bit operating system.

\subsubsection{End-to-end-DRL-for-FJSP}
Authors of \textbf{End-to-end-DRL-for-FJSP} trained the model on randomly generated instances with sizes 6x6, 10x10, 15x15, 20x10, 20x20, and 30x20. The number of GIN layers was set to $K = 2$. Each GIN layer has MLP with two hidden layers with dimension 128 \cite{LEI2022117796}. Job operation action selection decoder, machine action selection decoder, and state-value function all have two hidden layers with dimension 128. Adam optimizer with learning rate $10^{-3}$ was used to update the policy.
\par
We obtained only one parameter set from the GitHub repository \cite{github_end_to_end_drl_for_fjsp}.

\subsubsection{fjsp-drl}
Authors of \textbf{fjsp-drl} trained the model on randomly generated instances with sizes 10x5, 20x5, 15x10, and 20x10 \cite{9826438}. The model was evaluated on instances with a size of 30x10 and 40x10. The number of HGNN layers was set to $K = 3$, and the dimensions of operation and machine embeddings were set to 8. Five networks $\text{MLP}_{\theta_i}$ have two hidden layers with dimension 128, actor-network $\text{MLP}_\omega$ has two hidden layers with hidden dimension 64, and critic network also has two hidden layers with dimension 64. Hyperparameters for PPO were set to $I = 1000$ and $B = 20$. Adam optimizer with learning rate $2x10^{-4}$ was used for policy optimization. The discount factor was set to 1.0.
\par
We obtained five parameter sets from the GitHub repository for this model \cite{github_fjsp_drl}.

\subsection{Baselines}
We also compared previously described models with PDRs.
\par
\subsubsection*{JSSP}
For JSSP, we used PDR implementations from the code of the model \textbf{IEEE-ICCE-RL-JSP}. On top of FIFO, MOR, SPT, and MWKR discussed in \ref{priority dispatching rules}, we also used the following PDRs available in the code of \textbf{IEEE-ICCE-RL-JSP} \cite{github_ieee_icce_rl_jsp}:
\begin{enumerate}
    \item \textit{EDD} (Earliest Due Date)
    \item \textit{LOR} (Least Operations Remaining)
    \item \textit{LPT} (Longest Processing Time)
    \item \textit{LS} (Least Slack)
    \item \textit{SRPT} (Shortest Remaining Processing Time)
\end{enumerate}

\subsubsection*{FJSP}
For FJSP, we used PDR implementations from the code of the model \textbf{End-to-end-DRL-for-FJSP}. For operation selection, besides FIFO, we used three PDRs available in the code:
\begin{enumerate}
    \item \textit{MOPNR} (Most Operation Number Remaining)
    \item \textit{LWKR} (Least Work Remaining)
    \item \textit{MWKR} (Most Work Remaining)
\end{enumerate}
After selecting the operation, we used two machine selection PDRs available in the code:
\begin{enumerate}
    \item \textit{SPT} (Shortest Processing Time)
    \item \textit{EET} (Earliest End Time)
\end{enumerate}
All eight different combinations are used as baselines.

\section{Results}
We ran all experiments on a MacBook Pro with an Apple M2 Max chip, 32 GB of RAM, and Sonoma 14.4.1 macOS operating system. Besides reporting makespan and gap, we also report runtime for each experiment. 

\subsection{JSSP}
Average gaps for different models and instance categories are shown in Table 4.1. The model with the lowest average gap is highlighted in bold. Average runtimes are in Table 4.2. 

\begin{table}[H]
    Table 4.1: Average JSSP gaps for different models and categories\\
    \vspace{1mm}
    \small 
    \begin{tabular}{lllll}
        \toprule
        Model / Category & large & medium & small & all \\ 
        \midrule
        EDD & 0.30 ± 0.09 & 0.44 ± 0.11 & 0.35 ± 0.13 & 0.36 ± 0.12 \\
        FIFO & 0.28 ± 0.18 & 0.33 ± 0.11 & 0.25 ± 0.09 & 0.30 ± 0.13  \\
        LOR & 0.35 ± 0.09 & 0.44 ± 0.11 & 0.36 ± 0.12 & 0.40 ± 0.11\\
        LPT & 0.42 ± 0.11 & 0.48 ± 0.11 & 0.36 ± 0.10 & 0.44 ± 0.11\\
        LRPT & 0.19 ± 0.13 & 0.27 ± 0.11 & 0.17 ± 0.08 & 0.22 ± 0.12 \\
        LS & 0.24 ± 0.10 & 0.34 ± 0.09 & 0.31 ± 0.14 & 0.29 ± 0.11 \\
        MOR & 0.26 ± 0.20 & 0.28 ± 0.12 & 0.20 ± 0.08 & 0.26 ± 0.14 \\
        SPT & 0.22 ± 0.07 & 0.30 ± 0.08 & 0.24 ± 0.10 & 0.26 ± 0.09 \\
        SRPT & 0.35 ± 0.09 & 0.46 ± 0.11 & 0.39 ± 0.11 & 0.40 ± 0.11 \\
        End-to-end-DRL-for-FJSP & 0.28 ± 0.18 & 0.33 ± 0.12 & 0.30 ± 0.14 & 0.30 ± 0.14  \\
        fjsp-drl & 0.20 ± 0.15 & 0.25 ± 0.11 & 0.16 ± 0.08 & 0.22 ± 0.12 \\
        \textbf{IEEE-ICCE-RL-JSP} & \textbf{0.18 ± 0.13} & \textbf{0.24 ± 0.09} & \textbf{0.16 ± 0.07} & \textbf{0.20 ± 0.12} \\
        L2D & 0.28 ± 0.19 & 0.35 ± 0.12 & 0.25 ± 0.11 & 0.31 ± 0.16 \\
        Wheatley & 0.79 ± 0.59 & 0.43 ± 0.18 & 0.26 ± 0.11 & 0.48 ± 0.38 \\
        \bottomrule
    \end{tabular}
\end{table}


\begin{table}[H]
    Table 4.2: Average JSSP runtimes for different models and categories\\
    \vspace{1mm}
    \small
    \begin{tabular}{lllll}
        \toprule
        category & large & medium & small & all \\
        model &  &  &  &  \\
        \midrule
        EDD & 8.13 ± 7.10 & 0.75 ± 0.62 & 0.06 ± 0.05 & 4.04 ± 6.11 \\
        FIFO & 4.99 ± 5.67 & 0.70 ± 0.61 & 0.06 ± 0.05 & 1.69 ± 3.51 \\
        LOR & 3.26 ± 3.70 & 0.47 ± 0.40 & 0.04 ± 0.04 & 1.09 ± 2.26 \\
        LPT & 3.84 ± 4.20 & 0.70 ± 0.51 & 0.06 ± 0.05 & 2.20 ± 3.39 \\
        LRPT & 5.00 ± 4.86 & 0.60 ± 0.49 & 0.05 ± 0.04 & 2.31 ± 3.82 \\
        LS & 4.76 ± 4.72 & 0.52 ± 0.44 & 0.04 ± 0.03 & 2.43 ± 3.88 \\
        MOR & 3.83 ± 4.18 & 0.53 ± 0.46 & 0.05 ± 0.04 & 1.26 ± 2.58 \\
        SPT & 3.71 ± 4.20 & 0.60 ± 0.47 & 0.06 ± 0.04 & 2.10 ± 3.37 \\
        SRPT & 4.86 ± 4.58 & 0.46 ± 0.39 & 0.05 ± 0.03 & 2.41 ± 3.82 \\
        End-to-end-DRL-for-FJSP & 10.90 ± 11.96 & 1.74 ± 1.14 & 0.36 ± 0.20 & 4.97 ± 8.69 \\
        fjsp-drl & 49.13 ± 65.99 & 7.69 ± 5.22 & 1.14 ± 0.90 & 16.70 ± 38.10 \\
        IEEE-ICCE-RL-JSP & 16.26 ± 14.29 & 3.74 ± 2.16 & 0.58 ± 0.29 & 9.85 ± 12.17 \\
        L2D & 4.45 ± 3.99 & 1.10 ± 0.54 & 0.26 ± 0.14 & 2.46 ± 3.18 \\
        Wheatley & 32.34 ± 42.37 & 5.02 ± 3.56 & 0.89 ± 0.56 & 10.98 ± 24.60 \\
        \bottomrule
        \end{tabular}        
\end{table}

A boxplot for JSSP gaps for small instances is shown in Figure 4.1, for medium instances in Figure 4.2, and large instances in Figure 4.3. The model with the lowest average gap is highlighted as a blue box.

\begin{center}
    \includegraphics[width=0.8\linewidth]{images/horizontal_boxplot_jssp_small.pdf}\\
    Figure 4.1: JSSP gaps for small instances
\end{center}

\begin{center}
    \includegraphics[width=0.8\linewidth]{images/horizontal_boxplot_jssp_medium.pdf}\\
    Figure 4.2: JSSP gaps for medium instances
\end{center}
\begin{center}
    \includegraphics[width=0.8\linewidth]{images/horizontal_boxplot_jssp_large.pdf}\\
    Figure 4.3: JSSP gaps for large instances
\end{center}
\begin{center}
    \includegraphics[width=0.8\linewidth]{images/horizontal_boxplot_jssp_all.pdf}\\
    Figure 4.4: JSSP gaps for all instances
\end{center}

To test the null hypothesis that the median values of all models and PDRs are from the same population, we used the Kruskal-Wallis test \cite{doi:10.1080/01621459.1952.10483441}. The p-value for JSSP gaps, including all models and PDRs in Table 4.1, is $p < 1\%$ for all sizes. We, therefore, reject the null hypothesis that all models and PDRs are equal. Without PDRs, the p-value is also $p < 1\%$. We reject the null hypothesis that all models are equal, too.
\par
From Table 4.1 and Figures 4.1, 4.2, 4.3, and 4.4, we observed that priority dispatching rule \textbf{LRPT} and model \textbf{fjsp-drl} produced similar results as the model \textbf{IEEE-ICCE-RL-JSP}, which had the lowest mean gap. Therefore, we tested the null hypothesis that \textbf{LRPT}, \textbf{fjsp-drl}, and \textbf{IEEE-ICCE-RL-JSP} are from the same population. The p-value for small instances is $45.8\%$, for medium instances $1.1\%$, $3.8\%$ for large instances, and 0.1$\%$ for all instances. We, therefore, reject the null hypothesis that these three models are from the same population for medium and large instances.
For each pair of these three models, we then tested the null hypothesis using the Mann-Whitney \textit{U} test: that the pair of models was equal. Obtained p-values are shown in Table 4.3.

\begin{table}[H]
    Table 4.3: p-values for Mann-Whitney \textit{U} test between pairs of models\\
    \vspace{1mm}
    \begin{tabular}{llr}
    \toprule
    Category & Models & p-value \\
    \midrule
    large & LRPT + fjsp-drl & 85.1$\%$ \\
    large & LRPT + IEEE-ICCE-RL-JSP & 3.0$\%$ \\
    large & fjsp-drl + IEEE-ICCE-RL-JSP & 10.8$\%$ \\
    medium & LRPT + fjsp-drl & 0.6$\%$ \\
    medium & LRPT + IEEE-ICCE-RL-JSP & 0.4$\%$ \\
    medium & fjsp-drl + IEEE-ICCE-RL-JSP & 61.6$\%$ \\
    small & LRPT + fjsp-drl & 48.3$\%$ \\
    small & LRPT + IEEE-ICCE-RL-JSP & 22.7$\%$ \\
    small & fjsp-drl + IEEE-ICCE-RL-JSP & 48.0$\%$ \\
    all & LRPT + fjsp-drl & 48.3$\%$ \\
    all & LRPT + IEEE-ICCE-RL-JSP & 22.7$\%$ \\
    all & fjsp-drl + IEEE-ICCE-RL-JSP & 48.0$\%$ \\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{FJSP}


% \begin{tabular}{llll}
%     \toprule
%     category & large & medium & small \\
%     parameter-set &  &  &  \\
%     \midrule
%     DQN-ep1400 & 0.18 ± 0.12 & 0.24 ± 0.09 & 0.16 ± 0.07 \\
%     DQN-ep1530 & 0.18 ± 0.13 & 0.25 ± 0.09 & 0.15 ± 0.06 \\
%     DQN-ep1670 & 0.18 ± 0.13 & 0.24 ± 0.09 & 0.16 ± 0.08 \\
%     DQN-ep2590 & 0.18 ± 0.12 & 0.24 ± 0.09 & 0.16 ± 0.07 \\
%     DQN-ep3480 & 0.18 ± 0.14 & 0.24 ± 0.11 & 0.15 ± 0.07 \\
%     EDD & 0.30 ± 0.09 & 0.44 ± 0.11 & 0.35 ± 0.13 \\
%     FIFO & 0.28 ± 0.18 & 0.33 ± 0.11 & 0.25 ± 0.09 \\
%     LOR & 0.35 ± 0.09 & 0.44 ± 0.11 & 0.36 ± 0.12 \\
%     LPT & 0.42 ± 0.11 & 0.48 ± 0.11 & 0.36 ± 0.10 \\
%     LRPT & 0.19 ± 0.13 & 0.27 ± 0.11 & 0.17 ± 0.08 \\
%     LS & 0.24 ± 0.10 & 0.34 ± 0.09 & 0.31 ± 0.14 \\
%     MOR & 0.26 ± 0.20 & 0.28 ± 0.12 & 0.20 ± 0.08 \\
%     SPT & 0.22 ± 0.07 & 0.30 ± 0.08 & 0.24 ± 0.10 \\
%     SRPT & 0.35 ± 0.09 & 0.46 ± 0.11 & 0.39 ± 0.11 \\
%     SavedNetwork/10-10-1-99.pth & 0.28 ± 0.19 & 0.32 ± 0.13 & 0.24 ± 0.11 \\
%     SavedNetwork/15-15-1-99.pth & 0.30 ± 0.21 & 0.33 ± 0.13 & 0.23 ± 0.09 \\
%     SavedNetwork/20-15-1-199.pth & 0.25 ± 0.18 & 0.34 ± 0.12 & 0.22 ± 0.10 \\
%     SavedNetwork/20-15-1-99.pth & 0.32 ± 0.21 & 0.33 ± 0.12 & 0.24 ± 0.11 \\
%     SavedNetwork/20-20-1-199.pth & 0.29 ± 0.20 & 0.34 ± 0.12 & 0.24 ± 0.11 \\
%     SavedNetwork/20-20-1-99.pth & 0.29 ± 0.19 & 0.35 ± 0.12 & 0.23 ± 0.09 \\
%     SavedNetwork/30-15-1-199.pth & 0.28 ± 0.18 & 0.37 ± 0.12 & 0.28 ± 0.11 \\
%     SavedNetwork/30-15-1-99.pth & 0.27 ± 0.19 & 0.33 ± 0.13 & 0.25 ± 0.10 \\
%     SavedNetwork/30-20-1-199.pth & 0.27 ± 0.18 & 0.37 ± 0.11 & 0.31 ± 0.11 \\
%     SavedNetwork/30-20-1-99.pth & 0.25 ± 0.17 & 0.33 ± 0.12 & 0.23 ± 0.09 \\
%     SavedNetwork/6-6-1-99.pth & 0.28 ± 0.15 & 0.39 ± 0.09 & 0.30 ± 0.14 \\
%     model/save-10-5.pt & 0.18 ± 0.13 & 0.24 ± 0.11 & 0.16 ± 0.08 \\
%     results/save-10-5.pt & 0.18 ± 0.13 & 0.24 ± 0.11 & 0.16 ± 0.08 \\
%     results/save-15-10.pt & 0.18 ± 0.12 & 0.24 ± 0.09 & 0.16 ± 0.09 \\
%     results/save-20-10.pt & 0.24 ± 0.19 & 0.28 ± 0.12 & 0.18 ± 0.09 \\
%     results/save-20-5.pt & 0.20 ± 0.17 & 0.24 ± 0.11 & 0.16 ± 0.08 \\
%     saved-models/1 & 0.57 ± 0.23 & 0.41 ± 0.15 & 0.27 ± 0.11 \\
%     saved-models/2& 0.74 ± 0.41 & 0.45 ± 0.18 & 0.27 ± 0.11 \\
%     saved-models/3 & 1.06 ± 0.85 & 0.42 ± 0.21 & 0.24 ± 0.10 \\
%     saved-network/FJSP-J15M15/best-value0 & 0.28 ± 0.18 & 0.33 ± 0.12 & 0.24 ± 0.09 \\
%     \bottomrule
% \end{tabular}
