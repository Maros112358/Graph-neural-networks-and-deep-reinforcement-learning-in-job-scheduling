\chapter{Reinforcement learning with graph neural networks}
\label{chap:math}

Deep learning and neural networks have achieved unprecedented success since their introduction and currently being state-of-the-art in numerous fields, such as object detection \cite{DBLP:journals/corr/RedmonDGF15, 10.1109/IVS.2019.8813777, 8627998}, machine translation \cite{DBLP:journals/corr/LuongPM15, 8003957, DBLP:journals/corr/abs-2002-07526}, and many others \cite{DONG2021100379, 10.1145/3505243, PICCIALLI2021111}. Many deep learning techniques involve learning from Euclidian data (e.g., images, text, and videos). At the same time, there is an increasing number of fields where data is represented as graphs. For example, the interaction of users on social media \cite{10.1145/3308558.3313488}, atoms and their bonds in protein molecules \cite{strokach2020fast}, and traffic forecasting \cite{JIANG2022117921}. Learning from graph data has created significant challenges. Graphs can be irregular, with varying numbers of nodes, and each node can have a different number of neighbors. As a result, some important operations (e.g., convolution) can not be applied the same way as in the case of images. With growing interest in deep learning from graph data, new methods motivated by Convolutional Neural Networks (CNNs) and Recurrent Neural networks (RNNs) have been developed. For example, an image can be thought of as a graph, where each node is a pixel, and edges connect nodes partaking in convolution, as illustrated in Figure 2.1 below \cite{9046288}.\\
\begin{center}
    \includegraphics[width=0.6\linewidth]{images/image_vs_graph.pdf}\\
    Figure 2.1: 2-D Convolution versus graph convolution \cite{9046288}
\end{center}
\newpage
Similarly, for text processing and RNNs, text can be thought of as a graph, where each node is a word in a sentence, and every two consecutive words are connected via an oriented arc, as illustrated in Figure 2.2 below \cite{sanchez-lengeling2021a}.
\begin{center}
    \includegraphics[width=0.7\linewidth]{images/graph_are_all_around_us.pdf}\\
    Figure 2.2: Text as a graph, reproduced from \cite{sanchez-lengeling2021a}
\end{center}

\section{Graph Neural Networks}

In the simplest case, when data is represented as a non-oriented graph $G = (O, E)$, each node $o \in O$ and each edge $(o, w) \in E$ has a seat of features $x_o$ and $y_{ow}$ respectively. For example, the processing time of each operation node in the JSSP disjunctive graph. 
\par
Simple GNN takes this graph as an input and processes its features in two phases. The first phase called \textit{message passing phase} \cite{pmlr-v70-gilmer17a}, runs for $T$ time steps and uses a message function $M_t$ and node update function $U_t$. In each time step $t$, for each node $o$, a message $m_o^{t + 1}$ is calculated, and the embedding of the node $h_o^{t}$ is updated according to \cite{pmlr-v70-gilmer17a}
\begin{equation}\label{equation:2.1}
	m_o^{(t + 1)} = \sum_{w \in \mathcal{N}(o)} M_{(t)} (h_o^{(t)}, h_w^{(t)}, y_{ow}) \, ,
\end{equation}
\begin{equation}\label{equation:2.2}
	h_o^{(t+1)} = U_{(t)} (h_o^{(t)}, m_o^{(t+1)})
\end{equation}
where $\mathcal{N}(o)$ in the sum defines the neighbourhood of the node $o$ in graph $G$. The initial embedding of the node is $h_o^{(0)} = x_o$. Each timestep is often called a \textit{layer} of GNN.
\par
In the \textit{readout phase}, a readout function $R$ computes a feature vector $z_o$ from obtained node embeddings \cite{pmlr-v70-gilmer17a}
\begin{equation}\label{equation:2.3}
	z_o = R(h_o^{(T)}) \hspace{2em} \forall o \in O \, .
\end{equation}
Message function $M_{(t)}$, node update function $U_{(t)}$, and readout function $R$ can all be learned differentiable functions\\
Different forms of equations \ref{equation:2.1}, \ref{equation:2.2}, and \ref{equation:2.3} lead to different types of GNNs performing different tasks, for example \cite{10.1145/3495161, sanchez-lengeling2021a}:
\begin{itemize}
	\item Graph Convolutional Neural Networks (GCNNs)
	\item Graph Recurrent Neural Networks (GRNNs)
	\item Graph Attention Networks (GATs)
\end{itemize} 
This thesis will focus primarily on relevant GNN architectures used in available job scheduling models described in the next chapter, specifically Graph Isomorphism Network (GIN) and GAT.

\subsection{Graph Isomorphism Network}
Graph Isomorphism Network (GIN) is a GNN variant achieving maximum discriminative power \cite{DBLP:journals/corr/abs-1810-00826}. Given undirected graph $G = (O, E)$, in the message passing phase, the update function $U_{(t)}$ is represented by a multi-layer perceptron $\text{MLP}^{(t)}$ and \ref{equation:2.2} has the following form \cite{DBLP:journals/corr/abs-1810-00826}
\begin{equation} \label{equation:2.4}
	h_o^{(t+1)} = \text{MLP}^{(t)} \left ( \left ( 1 + \epsilon^{(t)} \right ) h_o^{(t-1)} + \sum_{w\in \mathcal{N}(o)} h_w^{(t - 1)} \right ) \, ,
\end{equation}
where $\epsilon^{(t)}$ can be learned or a fixed scalar \cite{DBLP:journals/corr/abs-1810-00826}. In the readout phase, the readout function $R$ depends on the model's given task, e.g., node classification, link prediction, and graph classification \cite{DBLP:journals/corr/abs-1810-00826}. The next chapter will specify specific forms of readout function for the given models.
\par
A straightforward strategy to extend GIN to directed graphs $G = (O, A)$ is to define the neighborhood of node $o \in O$ as $\mathcal{N}(o) = \{ w | \ (w, o) \in A\}$, i.e., all incoming neighbors of $o$ \cite{zhang2020learning}.\\
\\
For GIN on heterogeneous graphs, the equation \ref{equation:2.4} is applied separately for each type of neighbor node and then combined \cite{pytorch_hetero_conv, 10226873}. Let $a$ denote a type of node $o$, $b_i$ denote a type of node $w_i$, and $r_i$ be an edge type connecting nodes of types $a$ and $b_i$. Then, let neighborhood $\mathcal{N}_{r_i}(o)$ be a set of nodes connected to $o$ by an edge of type $r_i$. Then, for each edge type $r_i$, updated embedding $h_{o, r_i}^{(t+1)}$ of node $o$ is calculated by equation \ref{equation:2.4} separately (with separate multi-layer perceptrons) as follows \cite{pytorch_hetero_conv}
\begin{equation} \label{equation:2.5}
	h_{o, r_i}^{(t+1)} = \text{MLP}^{(t)}_{r_i} \left ( \left ( 1 + \epsilon_{r_i}^{(t)} \right ) h_{o}^{(t-1)} + \sum_{w\in \mathcal{N}_{r_i}(o)} h_{w}^{(t - 1)} \right ) \, ,
\end{equation}
where $\text{MLP}^{(t)}_{r_i}$ is a multilayer perceptron for given edge type \cite{pytorch_hetero_conv}. New embeddings for the node $o$ for each edge type $r_i$ are then combined via some aggregation function \cite{pytorch_hetero_conv}.