
\chapwithtoc{Introduction}

Job scheduling considers the optimal allocation of the number of jobs consisting of multiple operations with predefined constraints (e.g., operations are processed in a 
specific order) to a set of shared resources to minimize total makespan, tardiness, cost, or other objectives \cite{YamadaNakanoJSSP}. Each resource can process only one operation at a time and is non-preemptive, i.e., it runs without interruption.
\par
Different constraints and conditions lead to different variants of job scheduling. 
Job shop scheduling problem (JSSP) assumes all jobs to be known apriori and that each operation can be allocated only to one given machine \cite{YamadaNakanoJSSP}. In flexible job-shop scheduling (FJSP), each operation can be allocated to any of the machines from a given subset of machines \cite{DAUZEREPERES2024409}. Dynamic job-shop scheduling (DJSP), one of the more common variants at the time, tackles the stochastic aspects of modern manufacturing, e.g., the arrival of new jobs during the execution of the schedule or uncertain processing time \cite{MOHAN201934}.
\par
Due to the NP-hardness of these problems \cite{Garey1976TheCO}, numerous approaches and heuristics have been proposed over time to yield approximate solutions \cite{Jansen2000ApproximationAF}. Priority dispatching rule (PDR) \cite{Haupt1989ASO} is a computationally fast and intuitive heuristic method compared to other optimization methods and is widely used in scheduling systems. Many different PDRs have been proposed and extensively studied in the literature \cite{doi:10.1080/00207543.2011.611539}. Designing a quality PDR is usually a very time-consuming task requiring extensive domain knowledge. Deep reinforcement learning (DRL) has already been proposed as a possible solution for the automatization of algorithm learning for combinatorial optimization problems (COPs) \cite{bengio2020machine}. Several recent works have focused on extending this technique to job scheduling \cite{zhang2020learning, https://doi.org/10.1002/tee.23788, DBLP:journals/corr/abs-2106-01086, 10114974, 9826438, 10226873} applying graph neural networks (GNNs) on a graph representation of job scheduling problems \cite{BLAZEWICZ2000317}. 

\xxx{After the rest of the chapters is added, I will add the description of each chapter here.}

% Introduction should answer the following questions, ideally in this order:
% \begin{enumerate}
% \item What is the nature of the problem the thesis is addressing?
% \item What is the common approach for solving that problem now?
% \item How this thesis approaches the problem?
% \item What are the results? Did something improve?
% \item What can the reader expect in the individual chapters of the thesis?
% \end{enumerate}

% Expected length of the introduction is between 1--4 pages. Longer introductions may require sub-sectioning with appropriate headings --- use \texttt{\textbackslash{}section*} to avoid numbering (with section names like `Motivation' and `Related work'), but try to avoid lengthy discussion of anything specific. Any ``real science'' (definitions, theorems, methods, data) should go into other chapters.
% \todo{You may notice that this paragraph briefly shows different ``types'' of `quotes' in TeX, and the usage difference between a hyphen (-), en-dash (--) and em-dash (---).}

% It is very advisable to skim through a book about scientific English writing before starting the thesis. I can recommend `\citetitle{glasman2010science}' by \citet{glasman2010science}.
