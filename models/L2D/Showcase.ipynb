{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "666d538e-30f1-490f-a3a9-f35884777192",
   "metadata": {},
   "source": [
    "# L2D Showcase\n",
    "\n",
    "The following code is a modified version of [test_learned_on_benchmark.py](l2d/test_learned_on_benchmark.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52a905db-715d-4e3b-be5a-72e81849a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"repo\")\n",
    "import os\n",
    "os.chdir(\"repo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02484b1a-6040-4e2e-940f-93415a13dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mb_agg import *\n",
    "from agent_utils import *\n",
    "from Params import configs\n",
    "from JSSP_Env import SJSSP\n",
    "from PPO_jssp_multiInstances import PPO\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74884603-3fff-4a27-ad8c-aa55642412fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_instance_taillard(filename):\n",
    "    '''Parses instance written in Taillard specification: http://jobshop.jjvh.nl/explanation.php\n",
    "    \n",
    "      Args:\n",
    "        filename - file containing the instance in Taillard specification\n",
    "\n",
    "      Returns:\n",
    "        number of jobs,\n",
    "        number of machines,\n",
    "        the processor times for each operation,\n",
    "        the order for visiting the machines\n",
    "    '''\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        # parse number of jobs J and machines M\n",
    "        J, M = map(int, f.readline().split())\n",
    "\n",
    "        # Initialize two empty numpy arrays with dimensions J x M\n",
    "        processor_times = np.empty((J, M), dtype=int)\n",
    "        orders_of_machines = np.empty((J, M), dtype=int)\n",
    "    \n",
    "        # Read the next J lines containing processor times\n",
    "        for i in range(J):\n",
    "            processor_times[i] = list(map(int, f.readline().split()))\n",
    "    \n",
    "        # Read the next J lines containing orders of machines\n",
    "        for i in range(J):\n",
    "            orders_of_machines[i] = list(map(int, f.readline().split()))\n",
    "\n",
    "        return J, M, processor_times, orders_of_machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bbd0137a-2875-4529-95ad-fd17272288da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_instance(instance: str, model: str, plan=None, device='cpu', machine_start_times=None, t:int=0):\n",
    "    '''Solves instance using given model and prints out the makespan\n",
    "    \n",
    "      Args:\n",
    "          instance - file with the instance in Taillard specification\n",
    "          model - network model to use as an agent\n",
    "\n",
    "      Returns:\n",
    "          makespan of the solution,\n",
    "          dispatch times of operations,\n",
    "          actions executed by the agent\n",
    "    '''\n",
    "    # parse the instance\n",
    "    jobs, machines, times, orders = parse_instance_taillard(instance)\n",
    "\n",
    "    # load agents environment\n",
    "    env = SJSSP(n_j=jobs, n_m=machines)\n",
    "    adj, fea, candidate, mask = env.reset((times, orders))\n",
    "    ep_reward = - env.max_endTime\n",
    "    if machine_start_times is not None:\n",
    "        env.mchsStartTimes = machine_start_times\n",
    "\n",
    "    # load the agent\n",
    "    ppo = PPO(configs.lr, configs.gamma, configs.k_epochs, configs.eps_clip,\n",
    "              n_j=jobs,\n",
    "              n_m=machines,\n",
    "              num_layers=configs.num_layers,\n",
    "              neighbor_pooling_type=configs.neighbor_pooling_type,\n",
    "              input_dim=configs.input_dim,\n",
    "              hidden_dim=configs.hidden_dim,\n",
    "              num_mlp_layers_feature_extract=configs.num_mlp_layers_feature_extract,\n",
    "              num_mlp_layers_actor=configs.num_mlp_layers_actor,\n",
    "              hidden_dim_actor=configs.hidden_dim_actor,\n",
    "              num_mlp_layers_critic=configs.num_mlp_layers_critic,\n",
    "              hidden_dim_critic=configs.hidden_dim_critic)\n",
    "    ppo.policy.load_state_dict(torch.load(model, map_location=torch.device('cpu')))\n",
    "    g_pool_step = g_pool_cal(graph_pool_type=configs.graph_pool_type,\n",
    "                             batch_size=torch.Size([1, env.number_of_tasks, env.number_of_tasks]),\n",
    "                             n_nodes=env.number_of_tasks,\n",
    "                             device=device)\n",
    "\n",
    "    # run the experiment\n",
    "    for i in itertools.count():\n",
    "        fea_tensor = torch.from_numpy(np.copy(fea)).to(device)\n",
    "        adj_tensor = torch.from_numpy(np.copy(adj)).to(device).to_sparse()\n",
    "        candidate_tensor = torch.from_numpy(np.copy(candidate)).to(device)\n",
    "        mask_tensor = torch.from_numpy(np.copy(mask)).to(device)\n",
    "\n",
    "        action = None\n",
    "        # choose action from partial plan if given\n",
    "        if plan is not None and i < len(plan):\n",
    "            env.set_current_time(plan[i][1])\n",
    "            action = np.int64(plan[i][0])\n",
    "            assert action in candidate, f\"Action {action} not in candidate {candidate}\" \n",
    "\n",
    "        # if no action was chosen from partial plan, use agent to choose\n",
    "        if action is None:\n",
    "            env.set_current_time(t)\n",
    "            with torch.no_grad():\n",
    "                pi, _ = ppo.policy(x=fea_tensor,\n",
    "                                   graph_pool=g_pool_step,\n",
    "                                   padded_nei=None,\n",
    "                                   adj=adj_tensor,\n",
    "                                   candidate=candidate_tensor.unsqueeze(0),\n",
    "                                   mask=mask_tensor.unsqueeze(0))\n",
    "                action = greedy_select_action(pi, candidate)\n",
    "\n",
    "\n",
    "        adj, fea, reward, done, candidate, mask = env.step(action)\n",
    "        ep_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    start_times = env.LBs - times\n",
    "    return env.posRewards - ep_reward, start_times, env.partial_sol_sequeence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc2f89a-0e94-4c6c-8b64-bd9191ff95ea",
   "metadata": {},
   "source": [
    "# 30x20 Taillard's instances\n",
    "I test the \"30x20\" model on Taillard's 30x20 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a906b80-fa8b-4037-9b9a-61b459fd6283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marosbratko/Graph-neural-networks-and-deep-reinforcement-learning-in-job-scheduling/models/L2D/repo/mb_agg.py:44: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:607.)\n",
      "  graph_pool = torch.sparse.FloatTensor(idx, elem,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Makespan of instance 'ta41.txt': 2592.0\n",
      "Makespan of instance 'ta42.txt': 2715.0\n",
      "Makespan of instance 'ta43.txt': 2431.0\n",
      "Makespan of instance 'ta44.txt': 2712.0\n",
      "Makespan of instance 'ta45.txt': 2651.0\n",
      "Makespan of instance 'ta46.txt': 2852.0\n",
      "Makespan of instance 'ta47.txt': 2502.0\n",
      "Makespan of instance 'ta48.txt': 2525.0\n",
      "Makespan of instance 'ta49.txt': 2497.0\n",
      "Makespan of instance 'ta50.txt': 2507.0\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"SavedNetwork/30_20_1_99.pth\"\n",
    "BENCHMARKS_PATH = \"../../../benchmarks/jssp/ta_instances/Taillard_specification/\"\n",
    "INSTANCES = [\n",
    "    'ta41.txt',\n",
    "    'ta42.txt',\n",
    "    'ta43.txt',\n",
    "    'ta44.txt',\n",
    "    'ta45.txt',\n",
    "    'ta46.txt',\n",
    "    'ta47.txt',\n",
    "    'ta48.txt',\n",
    "    'ta49.txt',\n",
    "    'ta50.txt',\n",
    "]\n",
    "\n",
    "for instance in INSTANCES:\n",
    "    makespan, _, _ = solve_instance(BENCHMARKS_PATH + instance, MODEL)\n",
    "    print(f\"Makespan of instance '{instance}': {makespan}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b931a6-f059-498e-a984-05e1a30b938e",
   "metadata": {},
   "source": [
    "# All benchmarks\n",
    "\n",
    "I test the \"30x20\" model on all available instances in Taillard specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c66e8aad-cae9-4ca0-8407-2cd9503fa64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_instances_in_taillard_specification():\n",
    "    '''Lists all instances in Taillard specification'''\n",
    "    matching_files = []\n",
    "    root_dir = \"../../../benchmarks/jssp/\"\n",
    "    target_string = \"Taillard_specification\"\n",
    "\n",
    "    for foldername, subfolders, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(foldername, filename)\n",
    "            if target_string in filepath:\n",
    "                matching_files.append(filepath)\n",
    "\n",
    "    return matching_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d029de1d-8fbf-415e-b046-180f1d2f7c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Makespan of instance 'dmu36.txt': 7308.0\n",
      "Makespan of instance 'dmu22.txt': 5606.0\n",
      "Makespan of instance 'dmu23.txt': 5267.0\n",
      "Makespan of instance 'dmu37.txt': 7364.0\n",
      "Makespan of instance 'dmu21.txt': 5706.0\n",
      "Makespan of instance 'dmu35.txt': 6457.0\n",
      "Makespan of instance 'dmu09.txt': 4377.0\n",
      "Makespan of instance 'dmu08.txt': 4071.0\n",
      "Makespan of instance 'dmu34.txt': 6514.0\n",
      "Makespan of instance 'dmu20.txt': 4715.0\n",
      "Makespan of instance 'dmu18.txt': 4908.0\n",
      "Makespan of instance 'dmu24.txt': 5626.0\n",
      "Makespan of instance 'dmu30.txt': 6130.0\n",
      "Makespan of instance 'dmu31.txt': 6971.0\n",
      "Makespan of instance 'dmu25.txt': 4945.0\n",
      "Makespan of instance 'dmu19.txt': 4862.0\n",
      "Makespan of instance 'dmu33.txt': 6155.0\n",
      "Makespan of instance 'dmu27.txt': 6140.0\n",
      "Makespan of instance 'dmu26.txt': 6089.0\n",
      "Makespan of instance 'dmu32.txt': 6572.0\n",
      "Makespan of instance 'dmu55.txt': 6582.0\n",
      "Makespan of instance 'dmu41.txt': 4738.0\n",
      "Makespan of instance 'dmu69.txt': 8729.0\n",
      "Makespan of instance 'dmu68.txt': 8713.0\n",
      "Makespan of instance 'dmu40.txt': 7122.0\n",
      "Makespan of instance 'dmu54.txt': 6299.0\n",
      "Makespan of instance 'dmu42.txt': 4940.0\n",
      "Makespan of instance 'dmu56.txt': 7034.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m instances \u001b[38;5;241m=\u001b[39m get_all_instances_in_taillard_specification()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m instance \u001b[38;5;129;01min\u001b[39;00m instances:\n\u001b[0;32m----> 4\u001b[0m     makespan, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43msolve_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMakespan of instance \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstance\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmakespan\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 60\u001b[0m, in \u001b[0;36msolve_instance\u001b[0;34m(instance, model, plan, device, machine_start_times, t)\u001b[0m\n\u001b[1;32m     58\u001b[0m     env\u001b[38;5;241m.\u001b[39mset_current_time(t)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 60\u001b[0m         pi, _ \u001b[38;5;241m=\u001b[39m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfea_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mgraph_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg_pool_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpadded_nei\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m                           \u001b[49m\u001b[43madj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madj_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcandidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcandidate_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m         action \u001b[38;5;241m=\u001b[39m greedy_select_action(pi, candidate)\n\u001b[1;32m     69\u001b[0m adj, fea, reward, done, candidate, mask \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/Graph-neural-networks-and-deep-reinforcement-learning-in-job-scheduling/models/L2D/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Graph-neural-networks-and-deep-reinforcement-learning-in-job-scheduling/models/L2D/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Graph-neural-networks-and-deep-reinforcement-learning-in-job-scheduling/models/L2D/repo/models/actor_critic.py:57\u001b[0m, in \u001b[0;36mActorCritic.forward\u001b[0;34m(self, x, graph_pool, padded_nei, adj, candidate, mask)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     49\u001b[0m             x,\n\u001b[1;32m     50\u001b[0m             graph_pool,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m             mask,\n\u001b[1;32m     55\u001b[0m             ):\n\u001b[0;32m---> 57\u001b[0m     h_pooled, h_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mgraph_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mpadded_nei\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadded_nei\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43madj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# prepare policy feature: concat omega feature with global feature\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     dummy \u001b[38;5;241m=\u001b[39m candidate\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_j, h_nodes\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/Graph-neural-networks-and-deep-reinforcement-learning-in-job-scheduling/models/L2D/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Graph-neural-networks-and-deep-reinforcement-learning-in-job-scheduling/models/L2D/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Graph-neural-networks-and-deep-reinforcement-learning-in-job-scheduling/models/L2D/repo/models/graphcnn_congForSJSSP.py:140\u001b[0m, in \u001b[0;36mGraphCNN.forward\u001b[0;34m(self, x, graph_pool, padded_nei, adj)\u001b[0m\n\u001b[1;32m    138\u001b[0m         h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_layer(h, layer, padded_neighbor_list\u001b[38;5;241m=\u001b[39mpadded_neighbor_list)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneighbor_pooling_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearn_eps:\n\u001b[0;32m--> 140\u001b[0m         h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAdj_block\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAdj_block\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m h_nodes \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# print(graph_pool.shape, h.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/Graph-neural-networks-and-deep-reinforcement-learning-in-job-scheduling/models/L2D/repo/models/graphcnn_congForSJSSP.py:109\u001b[0m, in \u001b[0;36mGraphCNN.next_layer\u001b[0;34m(self, h, layer, padded_neighbor_list, Adj_block)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# representation of neighboring and center nodes\u001b[39;00m\n\u001b[1;32m    108\u001b[0m pooled_rep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlps[layer](pooled)\n\u001b[0;32m--> 109\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norms\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpooled_rep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# non-linearity\u001b[39;00m\n\u001b[1;32m    112\u001b[0m h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(h)\n",
      "File \u001b[0;32m~/Graph-neural-networks-and-deep-reinforcement-learning-in-job-scheduling/models/L2D/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Graph-neural-networks-and-deep-reinforcement-learning-in-job-scheduling/models/L2D/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Graph-neural-networks-and-deep-reinforcement-learning-in-job-scheduling/models/L2D/venv/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Graph-neural-networks-and-deep-reinforcement-learning-in-job-scheduling/models/L2D/venv/lib/python3.10/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MODEL = \"SavedNetwork/30_20_1_99.pth\"\n",
    "instances = get_all_instances_in_taillard_specification()\n",
    "for instance in instances:\n",
    "    makespan, _, _ = solve_instance(instance, MODEL)\n",
    "    print(f\"Makespan of instance '{instance.split('/')[-1]}': {makespan}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f49545a-7c03-408b-a38f-7907b1670773",
   "metadata": {},
   "source": [
    "# Dynamic JSSP\n",
    "\n",
    "In dynamic JSSP, only a subset of jobs is known at the beginning. The rest of jobs arrives dynamically online."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdec98a-1841-45ec-8903-e474a4025dee",
   "metadata": {},
   "source": [
    "The following attempt to expand L2D to being dynamic is inspired by paper [Large-scale Dynamic Scheduling for Flexible Job-shop with Random Arrivals of New Jobs by Hierarchical Reinforcement Learning](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10114974), where authors schedule newly incoming jobs and reschedule not yet executed operations, already executed operations can not be rescheduled. During each rescheduling, they formulate static FJSP and solve it. They use cache for incoming jobs and an agent choosing either to add jobs from cache to scheduling problem, or keep them in cache. I will skip this agent and always add new jobs to scheduling problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d127e21d-1ee6-42e6-b29a-2981265283e8",
   "metadata": {},
   "source": [
    "Similarly to the paper, I will model the arrival of new jobs as poisson process with average arrival time following an exponential distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c5113019-8e9d-40a5-b0ce-139538af3a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_dynamics_jssp(instance):\n",
    "    '''Turns static JSSP instance to dynamic\n",
    "\n",
    "      Args:\n",
    "        filename of static JSSP instance\n",
    "\n",
    "      Returns:\n",
    "        list of jobs known at the beginning\n",
    "        dictionary of arriving jobs as  as {time_of_arrival: (operations, machines)} \n",
    "    '''\n",
    "    J, M, processor_times, orders_of_machines = parse_instance_taillard(instance)\n",
    "\n",
    "    indices = np.arange(J)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # separate jobs into known jobs and arriving jobs\n",
    "    jobs_known_at_the_beginning = [(processor_times[i], orders_of_machines[i]) for i in indices[J//2:]]\n",
    "    arriving_jobs_indeces = indices[:J//2]\n",
    "\n",
    "    # calculate beta = 1/lambda\n",
    "    average_time_between_arrivals = processor_times.mean()\n",
    "    \n",
    "    t = 0\n",
    "    arriving_jobs = {}\n",
    "    for index in arriving_jobs_indeces:\n",
    "        t += int(np.random.exponential(scale=average_time_between_arrivals)) + 1\n",
    "        arriving_jobs[t] = (processor_times[index], orders_of_machines[index])\n",
    "\n",
    "    return jobs_known_at_the_beginning, arriving_jobs\n",
    "\n",
    "def save_static_jssp_taillard(jobs):\n",
    "    '''Saves list of jobs as static JSSP instance in taillards specification\n",
    "        \n",
    "      Args:\n",
    "        list of jobs to save\n",
    "\n",
    "      Returns:\n",
    "        filename where JSSP instance was saved to\n",
    "    '''\n",
    "    J, M = len(jobs), len(jobs[0][0])\n",
    "    formatted_datetime = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    with open(f\"/tmp/{J}_{M}_{formatted_datetime}.txt\", 'w') as f:\n",
    "        f.write(f\"{J} {M}\\n\")\n",
    "        for job in jobs:\n",
    "            times, _ = job\n",
    "            f.write(\" \".join(map(str, times)) + '\\n')\n",
    "        for job in jobs:\n",
    "            _, orders = job\n",
    "            f.write(\" \".join(map(str, orders)) + '\\n')  \n",
    "\n",
    "    return f\"/tmp/{J}_{M}_{formatted_datetime}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "39bd0921-d93f-4da3-a6c8-3af8c44c29c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def solve_dynamic_jssp(instance, model):\n",
    "    '''Turns static JSSP in Taillard specification instance to dynamic and solves it\n",
    "\n",
    "      Args: \n",
    "        instance to solve\n",
    "\n",
    "      Returns: \n",
    "        makespan\n",
    "    '''\n",
    "    # turn static JSSP instance to dynamic\n",
    "    known_jobs, arriving_jobs = get_dynamics_jssp(instance)\n",
    "    # print(f\"instance={instance.split('/')[-1]}, known_jobs={len(known_jobs)}, arriving_jobs={len(arriving_jobs)}\")\n",
    "    latest_time_of_arrival = max(arriving_jobs)\n",
    "    # print(f\"Latest job arrives at {latest_time_of_arrival} and has total makespan {arriving_jobs[latest_time_of_arrival][0].max()}\")\n",
    "    \n",
    "    # solve static JSSP with jobs known initially\n",
    "    makespan, start_times, actions = solve_instance(save_static_jssp_taillard(known_jobs), model)\n",
    "    t = 0\n",
    "    plan = []\n",
    "    while True:\n",
    "        t += 1\n",
    "        \n",
    "        # no jobs left\n",
    "        if not arriving_jobs:\n",
    "            break\n",
    "    \n",
    "        # no job arrived\n",
    "        if not t in arriving_jobs:\n",
    "            continue\n",
    "    \n",
    "        # new job arrived, remove not yet executed operations from the plan\n",
    "        J, M = len(known_jobs), len(known_jobs[0][0])\n",
    "        for i in range(len(actions)):\n",
    "            row = actions[i] // M\n",
    "            col = actions[i] % M\n",
    "\n",
    "            if not start_times[row][col] < t:\n",
    "                continue\n",
    "                \n",
    "            if (actions[i], start_times[row][col]) in plan:\n",
    "                continue\n",
    "    \n",
    "            plan.append((actions[i], start_times[row][col]))\n",
    "\n",
    "        # add new job to the plan, with times shifted to current time t\n",
    "        new_job = arriving_jobs.pop(t)\n",
    "        known_jobs.append(new_job)\n",
    "        \n",
    "        # create new schedule WHILE REUSING THE ALREADY EXECUTED PLAN\n",
    "        makespan, start_times, actions = solve_instance(save_static_jssp_taillard(known_jobs), MODEL, plan=plan, t=t)\n",
    "\n",
    "    return makespan, plan, start_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7abc700-f69b-4702-8abe-48fb12c64276",
   "metadata": {},
   "source": [
    "## Run the dynamic experiment\n",
    "\n",
    "Run the experiment on 10 taillard instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "df8a1651-7f6e-44d7-91f3-5479935aa7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance=ta41.txt, static_makespan=2592.0, dynamic_makespan=3153.0\n",
      "instance=ta42.txt, static_makespan=2715.0, dynamic_makespan=2575.0\n",
      "instance=ta43.txt, static_makespan=2431.0, dynamic_makespan=2669.0\n",
      "instance=ta44.txt, static_makespan=2712.0, dynamic_makespan=2808.0\n",
      "instance=ta45.txt, static_makespan=2651.0, dynamic_makespan=2859.0\n",
      "instance=ta46.txt, static_makespan=2852.0, dynamic_makespan=2822.0\n",
      "instance=ta47.txt, static_makespan=2502.0, dynamic_makespan=2861.0\n",
      "instance=ta48.txt, static_makespan=2525.0, dynamic_makespan=2518.0\n",
      "instance=ta49.txt, static_makespan=2497.0, dynamic_makespan=2859.0\n",
      "instance=ta50.txt, static_makespan=2507.0, dynamic_makespan=2839.0\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"SavedNetwork/30_20_1_99.pth\"\n",
    "BENCHMARKS_PATH = \"../../../benchmarks/jssp/ta_instances/Taillard_specification/\"\n",
    "INSTANCES = [\n",
    "    'ta41.txt',\n",
    "    'ta42.txt',\n",
    "    'ta43.txt',\n",
    "    'ta44.txt',\n",
    "    'ta45.txt',\n",
    "    'ta46.txt',\n",
    "    'ta47.txt',\n",
    "    'ta48.txt',\n",
    "    'ta49.txt',\n",
    "    'ta50.txt',\n",
    "]\n",
    "\n",
    "for instance in INSTANCES:\n",
    "    static_makespan, _, _ = solve_instance(BENCHMARKS_PATH + instance, MODEL)\n",
    "    dynamic_makespan, plan, start_times = solve_dynamic_jssp(BENCHMARKS_PATH + instance, MODEL)\n",
    "    print(f\"instance={instance}, static_makespan={static_makespan}, dynamic_makespan={dynamic_makespan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adbf74f-b4a5-46f4-88d3-023ffe577953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
